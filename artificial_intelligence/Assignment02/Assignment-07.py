# -*- coding: utf-8 -*-
"""Assignment-07.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10OJdwn4XS7ZINc5w5firj9O1VdYDMIgY
"""

'''
 07. 8-8.py에 교차 검증을 적용해 보다 신뢰성 높은 성능을 제시하시오. (k=3)
교차 검증으로 측정한 정확률을 첨부하시오.
+7번 학습 시 epoch 값을 5으로 낮춰 적용하세요.
 '''
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,LSTM,Embedding
from tensorflow.keras import preprocessing
from tensorflow.keras.callbacks import EarlyStopping

dic_siz=10000 # 사전의 크기(사전에 있는 단어 개수)
sample_siz=512 # 샘플의 크기

# tensorflow가 제공하는 간소한 버전의 IMDB 읽기
(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=dic_siz)

embed_space_dim=16 # 16차원의 임베딩 공간

x_train=preprocessing.sequence.pad_sequences(x_train,maxlen=sample_siz)
x_test=preprocessing.sequence.pad_sequences(x_test,maxlen=sample_siz)

early=EarlyStopping(monitor='val_accuracy',patience=5,restore_best_weights=True)

# 신경망 모델의 설계와 학습(LSTM 층 포함)
embed=Sequential()
embed.add(Embedding(input_dim=dic_siz,output_dim=embed_space_dim,input_length=sample_siz))
embed.add(LSTM(units=32))
embed.add(Dense(1,activation='sigmoid'))
embed.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])
hist=embed.fit(x_train,y_train,epochs=5,batch_size=64,validation_split=0.2,verbose=2,callbacks=[early])

# 모델 평가
res=embed.evaluate(x_test,y_test,verbose=0)
print("정확률은",res[1]*100)

import matplotlib.pyplot as plt

# 학습 곡선
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train','Validation'], loc='best')
plt.grid()
plt.show()

from sklearn.model_selection import StratifiedKFold
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras import preprocessing
from tensorflow.keras.callbacks import EarlyStopping

dic_siz=10000 # 사전의 크기(사전에 있는 단어 개수)
sample_siz=512 # 샘플의 크기
k = 3 # 훈련 데이터를 3개로 분할

# tensorflow가 제공하는 간소한 버전의 IMDB 읽기
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=dic_siz)

embed_space_dim=16 # 16차원의 임베딩 공간

x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=sample_siz)
x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=sample_siz)

early = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)

# 3겹 교차 검증
skf = StratifiedKFold(n_splits=k, shuffle=True)
fold_accuracy = []

for train_index, val_index in skf.split(x_train, y_train):
    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # 신경망 모델의 설계와 학습(LSTM 층 포함)
    embed = Sequential()
    embed.add(Embedding(input_dim=dic_siz, output_dim=embed_space_dim, input_length=sample_siz))
    embed.add(LSTM(units=32))
    embed.add(Dense(1, activation='sigmoid'))
    embed.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])
    hist = embed.fit(x_train_fold, y_train_fold, epochs=5, batch_size=64, validation_data=(x_val_fold, y_val_fold),
                     verbose=2, callbacks=[early])

    _, accuracy = embed.evaluate(x_val_fold, y_val_fold) # 손실 값은 버림
    fold_accuracy.append(accuracy)

import numpy as np
import matplotlib.pyplot as plt

# 교차 검증으로 측정한 정확률
print('k개 정확률')
print(np.array(fold_accuracy)*100)
mean_accuracy = sum(fold_accuracy) / len(fold_accuracy)
print("평균 정확률은", mean_accuracy * 100)

# 학습 곡선
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train','Validation'], loc='best')
plt.grid()
plt.show()